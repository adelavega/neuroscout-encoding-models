{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized (banded) CV regression workflow for Neuroscout\n",
    "This notebook implements an encoding model for a single subject using Regularized Ridge Regression, as implemented in https://github.com/gallantlab/himalaya. In Neuroscout, this same pipeline should be run for all subjects.\n",
    "- Input needed from the user\n",
    "    - Define datasets (independent model fitting for all datasets)\n",
    "    - Define cross-validation strategy\n",
    "        - Across runs\n",
    "        - Within runs\n",
    "    - Define estimator\n",
    "    - Define preprocessing steps (e.g., scaling?)\n",
    "    - Define bands\n",
    "    - Pass parameters\n",
    "    - Output: scores, parameters, predicted time series\n",
    "- Define outputs\n",
    "\n",
    "<b> To do<b>:\n",
    "- Decide which outputs to store\n",
    "- Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/anaconda3/lib/python3.7/site-packages/nilearn/datasets/__init__.py:89: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pyns\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = pyns.Neuroscout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get fMRI data from datalad\n",
    "Let's retrieve data for a couple of subjects from Budapest.\n",
    "Neuroscout dataset can be found under the `neuroscout-datasets` organization: https://github.com/neuroscout-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path('/media/neuroscout-data/neuroscout/datasets/neuroscout-datasets/Budapest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !datalad get /media/neuroscout-data/neuroscout/datasets/neuroscout-datasets/Budapest/fmriprep/sub-sid000005/func/*-preproc_bold.nii.gz\n",
    "#!datalad get ds003017/fmriprep/sub-sid000007/func/*-preproc_bold.nii.gz\n",
    "#!datalad get ds003017/fmriprep/sub-sid000009/func/*-preproc_bold.nii.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR = 1\n",
    "subject = 'sid000005'\n",
    "sub_id = 'Budapest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Replace this with pybids logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_outputs(dataset_path, sub_id=None, TR=1):\n",
    "    arrays = []\n",
    "    fs = dataset_path.glob(f'fmriprep/sub-{sub_id}/func/*-preproc_bold.nii.gz')\n",
    "    fs = [str(f) for f in fs]\n",
    "    runs = sorted(set([f.split('/')[-1].split('_')[2].split('-')[1] for f in fs]))\n",
    "    resampling_ts = []\n",
    "    for r_ix, r in enumerate(runs):\n",
    "        img_file = [f for f in fs if f'run-{r}' in f][0]\n",
    "        data = np.asanyarray(nib.load(img_file).dataobj)\n",
    "        run_y = data.reshape([data.shape[0] * data.shape[1] * data.shape[2], data.shape[3]]).T\n",
    "        arrays.append(run_y)\n",
    "        resampling_ts.append(np.arange(0, TR*run_y.shape[0], step=TR))\n",
    "    return np.vstack(arrays), resampling_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, resampling_ts = _make_outputs(dataset_path, sub_id=subject, TR=TR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3052, 565812)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3052"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([t.shape[0] for t in resampling_ts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build input matrix\n",
    "Let's retrieve predictor events for multiple sets of predictors. \\\n",
    "For now, let's pick three sets: <b>Audioset</b> + <b>MFCC</b> + <b>mel</b> features (plus some confounds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioset = ['as-Music',\n",
    "            'as-Animal',\n",
    "            'as-Whistling',\n",
    "            'as-Vehicle',\n",
    "            'as-Wild animals',\n",
    "            'as-Thunderstorm',\n",
    "            'as-Noise',\n",
    "            'as-Fire',\n",
    "            'as-Water',\n",
    "            'as-Wind',\n",
    "            'as-Glass',\n",
    "            'as-Wood',\n",
    "            'as-Silence',\n",
    "            'as-Mechanisms',\n",
    "            'as-Alarm',\n",
    "            'as-Hands',\n",
    "            'as-Tools',\n",
    "            'as-Speech',\n",
    "            'as-Explosion',\n",
    "            'as-Engine',\n",
    "            'as-Liquid',\n",
    "            'as-Musical instrument']\n",
    "mfccs = [f'mfcc_{i}' for i in range(20)]\n",
    "mel = [f'mel_{i}' for i in range(64)]\n",
    "confounds = ['rot_x', 'rot_y', 'rot_z', 'trans_x', 'trans_y', 'trans_z',\n",
    "             'a_comp_cor_00', 'a_comp_cor_01', 'a_comp_cor_02',\n",
    "             'a_comp_cor_03','a_comp_cor_04','a_comp_cor_05']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that does naive resampling of predictor events to TR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that takes a list of lists (`predictor_sets`), a dataset name, and a subject id and returns:\n",
    "- `mats`, a list of pandas DataFrame (one per predictor set) of shape $n\\_TRs x n\\_features$;\n",
    "- `run_index`, a list of the same length as `mats`, with run_ids for each row in `mats`.\n",
    "Both `mats` and `run_index` are obtained by concatenating multiple runs. \\\n",
    "Each element of `predictor_sets` is a list of predictor names (e.g., `as-Speech`, `as-Music`). \\\n",
    "We also pass `resampling_ts` - not relevant for Neuroscout implementation, only needed for **ad hoc** resampling in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bids.variables import SparseRunVariable, BIDSRunVariableCollection\n",
    "from bids.variables.entities import RunInfo\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Given a combined subject_df, group_by predictor / run, resample to TR, and combin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_predictors_as_collections(predictor_name, dataset_name, **entities):\n",
    "    all_df = api.predictor_events.get(\n",
    "        predictor_name=predictor_name, dataset_name=dataset_name, output_type='df', **entities)\n",
    "    all_df = all_df.rename(columns={'number': 'run'})\n",
    "    \n",
    "    all_run_info = {}\n",
    "    for r_id in all_df.run_id.unique():\n",
    "        resp = api.runs.get(r_id)\n",
    "        all_run_info[r_id] = {\n",
    "            'duration': resp['duration'],\n",
    "            'tr': api.tasks.get(resp['task'])['TR']\n",
    "        }\n",
    "        \n",
    "    variables = []\n",
    "    for (run_id, predictor_id), df in all_df.groupby(['run_id', 'predictor_id']):\n",
    "        df = df.sort_values('onset')\n",
    "\n",
    "        name = df['predictor_name'].iloc[0]\n",
    "        meta_cols = ['subject', 'session', 'run', 'acquisition', 'run_id']\n",
    "        entities = {}\n",
    "        for j in meta_cols:\n",
    "            val = df[j].iloc[0]\n",
    "            if val:\n",
    "                entities[j] = val\n",
    "            else:\n",
    "                # Drop column from DF\n",
    "                df = df.drop(columns=j)\n",
    "        meta_cols = [m for m in meta_cols if m in df]\n",
    "\n",
    "        # TODO: Fetch real number of volumes, or allowing passing it in\n",
    "        this_run_info = all_run_info[entities['run_id']]\n",
    "        this_run_info['n_vols'] = math.ceil(this_run_info['duration'] / this_run_info['tr'])\n",
    "\n",
    "        run_info = RunInfo(**this_run_info, entities=entities, image=None)\n",
    "        df = df[['onset', 'duration', 'value'] + meta_cols].rename(columns={'value': 'amplitude'})\n",
    "\n",
    "        try:\n",
    "            df['amplitude'] = pd.to_numeric(df['amplitude'])\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        variables.append(SparseRunVariable(name, df, run_info, 'events'))\n",
    "            \n",
    "        \n",
    "    return BIDSRunVariableCollection(variables=variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = fetch_predictors_as_collections(\n",
    "    predictor_name=['as-Music', 'as-Speech'], dataset_name='Budapest', subject='sid000005')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_input_matrices(collection):\n",
    "    resampled = collection.to_dense().resample('TR')\n",
    "    df = resampled.to_df()\n",
    "    \n",
    "    keep = list(collection.variables.keys()) + ['onset', 'duration']\n",
    "    mat = df[keep]\n",
    "    metadata = df.drop(columns=keep)\n",
    "    \n",
    "    return mat, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats, idx = _make_input_matrices(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3052, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mats[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the encoding model with lots of comments and printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GroupKFold, PredefinedSplit\n",
    "from himalaya.ridge import GroupRidgeCV\n",
    "from himalaya.scoring import correlation_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validated model fitting, prediction, and scoring loosely based on scikit-learn's [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html). Returns a `results` dictionary with `'coefficients'`, `'test_predictions'`, and `'test_scores'` keys containing lists of numpy arrays for each outer cross-validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _model_cv(estimator, X, y, groups=None,\n",
    "              scoring=correlation_score, cv=None,\n",
    "              inner_cv=None, confounds=None, split=False):\n",
    "\n",
    "    # Container for results\n",
    "    results = {\n",
    "        'coefficients': [],\n",
    "        'test_predictions': [],\n",
    "        'test_scores': []}\n",
    "    \n",
    "    # Extract number of samples for convenience\n",
    "    n_samples = y.shape[0]\n",
    "    \n",
    "    # Keep count of confounds (if provided) for later\n",
    "    n_confounds = confounds.shape[1] if confounds is not None else 0\n",
    "    \n",
    "    # If confounds are provided, stack them onto X model\n",
    "    if confounds is not None:\n",
    "        X = X + [confounds] if type(X) == list else np.hstack((X, confounds))\n",
    "    \n",
    "    # Set default cross-validation to KFold if not specified\n",
    "    cv = KFold() if not cv else cv\n",
    "    \n",
    "    # Loop through outer cross-validation folds\n",
    "    for train, test in cv.split(np.arange(n_samples), groups=groups):\n",
    "        \n",
    "        # Get training model for list of model bands\n",
    "        X_train = [x[train] for x in X] if type(X) == list else X[train]\n",
    "        X_test = [x[test] for x in X] if type(X) == list else X[test]\n",
    "        \n",
    "        # Create inner cross-validation loop if specified\n",
    "        if inner_cv:\n",
    "            \n",
    "            # Split inner cross-validation with groups if supplied\n",
    "            inner_groups = np.array(groups)[train] if groups else groups\n",
    "            inner_splits = inner_cv.split(np.arange(n_samples)[train],\n",
    "                                          groups=inner_groups)\n",
    "            \n",
    "            # Update estimator with inner cross-validator\n",
    "            estimator.set_params(cv=inner_splits)\n",
    "            print(np.unique(inner_groups))\n",
    "        \n",
    "        # Fit the regression model on training data\n",
    "        estimator.fit(X_train, y[train])\n",
    "        \n",
    "        # Zero out coefficients for confounds if provided\n",
    "        if confounds is not None:\n",
    "            estimator.coef_[-n_confounds:] = 0\n",
    "        \n",
    "        # Compute predictions with optional splitting by band\n",
    "        test_prediction = estimator.predict(X_test, split=split)\n",
    "        \n",
    "        # Test scores should also optionally split by band\n",
    "        test_score = scoring(y[test], test_prediction)\n",
    "        \n",
    "        # Populate results dictionary\n",
    "        results['coefficients'].append(estimator.coef_)\n",
    "        results['test_predictions'].append(test_prediction)\n",
    "        results['test_scores'].append(test_score)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input data to numpy and fill NaNs\n",
    "X = [mat.fillna(0).to_numpy() for mat in mats]\n",
    "y = Y[:, :100]\n",
    "\n",
    "# Optionally create some dummy confounds\n",
    "confounds = np.random.randn(y.shape[0], 2)\n",
    "\n",
    "# Default estimator should be GroupRidgeCV\n",
    "estimator = GroupRidgeCV(groups='input')\n",
    "\n",
    "# Default cross-validation should be leave-one-run-out\n",
    "n_runs = len(np.unique(idx))\n",
    "cv = GroupKFold(n_splits=n_runs)\n",
    "inner_cv = GroupKFold(n_splits=n_runs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1433 1434 1435 1436]\n",
      "[........................................] 100% | 2.25 sec | 100 random sampling with cv | \n",
      "[1433 1434 1435 1437]\n",
      "[........................................] 100% | 2.56 sec | 100 random sampling with cv | \n",
      "[1434 1435 1436 1437]\n",
      "[........................................] 100% | 2.71 sec | 100 random sampling with cv | \n",
      "[1433 1434 1436 1437]\n",
      "[........................................] 100% | 2.81 sec | 100 random sampling with cv | \n",
      "[1433 1435 1436 1437]\n",
      "[........................................] 100% | 2.58 sec | 100 random sampling with cv | \n"
     ]
    }
   ],
   "source": [
    "# Run model with specified cross-validation, groups, confounds, and split outputs\n",
    "results = _model_cv(estimator, X, y, cv=cv, inner_cv=inner_cv, groups=idx, confounds=confounds, split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1433 1434 1435 1436]\n",
      "[........................................] 100% | 2.01 sec | 100 random sampling with cv | \n",
      "[1433 1434 1435 1437]\n",
      "[........................................] 100% | 2.28 sec | 100 random sampling with cv | \n",
      "[1434 1435 1436 1437]\n",
      "[........................................] 100% | 2.37 sec | 100 random sampling with cv | \n",
      "[1433 1434 1436 1437]\n",
      "[........................................] 100% | 2.49 sec | 100 random sampling with cv | \n",
      "[1433 1435 1436 1437]\n",
      "[........................................] 100% | 2.77 sec | 100 random sampling with cv | \n"
     ]
    }
   ],
   "source": [
    "results = _model_cv(estimator, X, y, cv=cv, inner_cv=inner_cv, groups=idx, confounds=confounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n",
      "[........................................] 100% | 2.57 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.67 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.38 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.73 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.68 sec | 100 random sampling with cv | \n"
     ]
    }
   ],
   "source": [
    "results = _model_cv(estimator, X, y, cv=KFold(), inner_cv=KFold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100% | 0.10 sec | 100 random sampling with cv | \n",
      "[..................................      ] 86% | 0.10 sec | 100 random sampling with cv | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100% | 0.12 sec | 100 random sampling with cv | \n",
      "[........................................] 100% | 0.11 sec | 100 random sampling with cv | \n",
      "[..................................      ] 85% | 0.09 sec | 100 random sampling with cv | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100% | 0.10 sec | 100 random sampling with cv | \n",
      "[........................................] 100% | 0.10 sec | 100 random sampling with cv | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "results = _model_cv(estimator, X, y, cv=KFold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n",
      "[........................................] 100% | 2.77 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.67 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.78 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.69 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.65 sec | 100 random sampling with cv | \n"
     ]
    }
   ],
   "source": [
    "results = _model_cv(estimator, X, y, inner_cv=KFold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100% | 0.10 sec | 100 random sampling with cv | \n",
      "[...................................     ] 89% | 0.09 sec | 100 random sampling with cv | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100% | 0.10 sec | 100 random sampling with cv | \n",
      "[........................................] 100% | 0.12 sec | 100 random sampling with cv | \n",
      "[............................            ] 71% | 0.08 sec | 100 random sampling with cv | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100% | 0.11 sec | 100 random sampling with cv | \n",
      "[........................................] 100% | 0.11 sec | 100 random sampling with cv | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "results = _model_cv(estimator, X, y, groups=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1433 1434 1435 1436]\n",
      "[1433 1434 1435 1437]\n",
      "[1434 1435 1436 1437]\n",
      "[1433 1434 1436 1437]\n",
      "[1433 1435 1436 1437]\n"
     ]
    }
   ],
   "source": [
    "# Using single-band (non-banded) model with sklearn RidgeCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "results = _model_cv(RidgeCV(), X[0], y, cv=cv, inner_cv=inner_cv, groups=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate against other workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

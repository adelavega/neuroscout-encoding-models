{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized (banded) CV regression workflow for Neuroscout\n",
    "This notebook implements an encoding model for a single subject using Regularized Ridge Regression, as implemented in https://github.com/gallantlab/himalaya. In Neuroscout, this same pipeline should be run for all subjects.\n",
    "- Input needed from the user\n",
    "    - Define datasets (independent model fitting for all datasets)\n",
    "    - Define cross-validation strategy\n",
    "        - Across runs\n",
    "        - Within runs\n",
    "    - Define estimator\n",
    "    - Define preprocessing steps (e.g., scaling?)\n",
    "    - Define bands\n",
    "    - Pass parameters\n",
    "    - Output: scores, parameters, predicted time series\n",
    "- Define outputs\n",
    "\n",
    "<b> To do<b>:\n",
    "- Decide which outputs to store\n",
    "- Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyns\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = pyns.Neuroscout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Budapest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acquisition': None,\n",
       " 'dataset_id': 27,\n",
       " 'duration': 535.0,\n",
       " 'id': 1435,\n",
       " 'number': 3,\n",
       " 'session': None,\n",
       " 'subject': 'sid000005',\n",
       " 'task': 48,\n",
       " 'task_name': 'movie'}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select subject from first run available in dataset\n",
    "api.runs.get(dataset_name='Budapest')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = api.runs.get(dataset_name='Budapest')[0]['subject']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch predictors from Neuroscout and create design matrix\n",
    "Let's retrieve predictor events for multiple sets of predictors. \\\n",
    "For now, let's pick three sets: <b>Audioset</b> + <b>MFCC</b> + <b>mel</b> features (plus some confounds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioset = ['as-Music',\n",
    "            'as-Animal',\n",
    "            'as-Whistling',\n",
    "            'as-Vehicle',\n",
    "            'as-Wild animals',\n",
    "            'as-Thunderstorm',\n",
    "            'as-Noise',\n",
    "            'as-Fire',\n",
    "            'as-Water',\n",
    "            'as-Wind',\n",
    "            'as-Glass',\n",
    "            'as-Wood',\n",
    "            'as-Silence',\n",
    "            'as-Mechanisms',\n",
    "            'as-Alarm',\n",
    "            'as-Hands',\n",
    "            'as-Tools',\n",
    "            'as-Speech',\n",
    "            'as-Explosion',\n",
    "            'as-Engine',\n",
    "            'as-Liquid',\n",
    "            'as-Musical instrument']\n",
    "mfccs = [f'mfcc_{i}' for i in range(20)]\n",
    "mel = [f'mel_{i}' for i in range(64)]\n",
    "confounds = ['rot_x', 'rot_y', 'rot_z', 'trans_x', 'trans_y', 'trans_z',\n",
    "             'a_comp_cor_00', 'a_comp_cor_01', 'a_comp_cor_02',\n",
    "             'a_comp_cor_03','a_comp_cor_04','a_comp_cor_05']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that does naive resampling of predictor events to TR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that takes a list of lists (`predictor_sets`), a dataset name, and a subject id and returns:\n",
    "- `mats`, a list of pandas DataFrame (one per predictor set) of shape $n\\_TRs x n\\_features$;\n",
    "- `run_index`, a list of the same length as `mats`, with run_ids for each row in `mats`.\n",
    "Both `mats` and `run_index` are obtained by concatenating multiple runs. \\\n",
    "Each element of `predictor_sets` is a list of predictor names (e.g., `as-Speech`, `as-Music`). \\\n",
    "We also pass `resampling_ts` - not relevant for Neuroscout implementation, only needed for **ad hoc** resampling in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bids.variables import SparseRunVariable, BIDSRunVariableCollection\n",
    "from bids.variables.entities import RunInfo\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Given a combined subject_df, group_by predictor / run, resample to TR, and combin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_predictors_as_collections(predictor_name, dataset_name, **entities):\n",
    "    # Fetch from API\n",
    "    all_df = api.predictor_events.get(\n",
    "        predictor_name=predictor_name, dataset_name=dataset_name, output_type='df', **entities)\n",
    "    all_df = all_df.rename(columns={'number': 'run', 'value': 'amplitude'})\n",
    "    \n",
    "    # Get run-level metadata\n",
    "    all_run_info = {}\n",
    "    for run_id in all_df.run_id.unique():\n",
    "        resp = api.runs.get(run_id)\n",
    "        ri = {\n",
    "            'duration': resp['duration'],\n",
    "            'tr': api.tasks.get(resp['task'])['TR'],\n",
    "            'image': None\n",
    "        }\n",
    "        \n",
    "        # TODO: Fetch real number of volumes, or allowing passing it in\n",
    "        ri['n_vols'] = math.ceil(ri['duration'] / ri['tr'])\n",
    "        all_run_info[run_id] = ri\n",
    "        \n",
    "    variables = []\n",
    "    for (run_id, predictor_name), df in all_df.groupby(['run_id', 'predictor_name']):\n",
    "        # Determine entities / run info\n",
    "        keep_cols = []\n",
    "        entities = {}\n",
    "        for j in ['subject', 'session', 'run', 'acquisition', 'run_id']:\n",
    "            val = df[j].iloc[0]\n",
    "            if val:\n",
    "                entities[j] = val\n",
    "                keep_cols.append(j)\n",
    "        run_info = RunInfo(**all_run_info[run_id], entities=entities)\n",
    "\n",
    "        try:\n",
    "            df['amplitude'] = pd.to_numeric(df['amplitude'])\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        df = df[['onset', 'duration', 'amplitude'] + keep_cols].sort_values('onset')\n",
    "        variables.append(SparseRunVariable(\n",
    "            predictor_name, df, run_info, 'events'))\n",
    "            \n",
    "    return BIDSRunVariableCollection(variables=variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "collection = fetch_predictors_as_collections(\n",
    "    predictor_name=['as-Music', 'as-Speech'], dataset_name=dataset_name, subject=subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_input_matrices(collection):\n",
    "    resampled = collection.to_dense().resample('TR')\n",
    "    df = resampled.to_df()\n",
    "    \n",
    "    keep = list(collection.variables.keys()) + ['onset', 'duration']\n",
    "    mat = df[keep]\n",
    "    metadata = df.drop(columns=keep)\n",
    "    \n",
    "    return mat, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats, idx = _make_input_matrices(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get fMRI data from datalad\n",
    "Let's retrieve data for a couple of subjects from Budapest.\n",
    "Neuroscout dataset can be found under the `neuroscout-datasets` organization: https://github.com/neuroscout-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datalad.api import install, get\n",
    "from bids.layout import BIDSLayout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dataset and fetch relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_datasets = Path('/media/neuroscout-data/neuroscout/datasets/neuroscout-datasets/')\n",
    "dataset_path = local_dataset_path / dataset_name\n",
    "dataset_url = 'https://github.com/neuroscout-datasets/ds003017.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro/anaconda3/lib/python3.7/site-packages/bids/layout/validation.py:51: UserWarning: The ability to pass arguments to BIDSLayout that control indexing is likely to be removed in future; possibly as early as PyBIDS 0.14. This includes the `config_filename`, `ignore`, `force_index`, and `index_metadata` arguments. The recommended usage pattern is to initialize a new BIDSLayoutIndexer with these arguments, and pass it to the BIDSLayout via the `indexer` argument.\n",
      "  warnings.warn(\"The ability to pass arguments to BIDSLayout that control \"\n",
      "/home/alejandro/anaconda3/lib/python3.7/site-packages/bids/layout/validation.py:156: UserWarning: The PipelineDescription field was superseded by GeneratedBy in BIDS 1.4.0. You can use ``pybids upgrade`` to update your derivative dataset.\n",
      "  warnings.warn(\"The PipelineDescription field was superseded \"\n"
     ]
    }
   ],
   "source": [
    "if not dataset_path.exists():\n",
    "    install(path=local_dataset_path, source=dataset_url)\n",
    "    \n",
    "layout = BIDSLayout(dataset_path / 'fmriprep', derivatives=dataset_path / 'fmriprep', index_metadata=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<BIDSImageFile filename='/media/neuroscout-data/neuroscout/datasets/neuroscout-datasets/Budapest/fmriprep/sub-sid000005/func/sub-sid000005_task-movie_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'>,\n",
       " <BIDSImageFile filename='/media/neuroscout-data/neuroscout/datasets/neuroscout-datasets/Budapest/fmriprep/sub-sid000005/func/sub-sid000005_task-movie_run-2_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'>,\n",
       " <BIDSImageFile filename='/media/neuroscout-data/neuroscout/datasets/neuroscout-datasets/Budapest/fmriprep/sub-sid000005/func/sub-sid000005_task-movie_run-3_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'>,\n",
       " <BIDSImageFile filename='/media/neuroscout-data/neuroscout/datasets/neuroscout-datasets/Budapest/fmriprep/sub-sid000005/func/sub-sid000005_task-movie_run-4_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'>,\n",
       " <BIDSImageFile filename='/media/neuroscout-data/neuroscout/datasets/neuroscout-datasets/Budapest/fmriprep/sub-sid000005/func/sub-sid000005_task-movie_run-5_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'>]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify functional runs\n",
    "layout.get(subject=subject, desc='preproc', extension='.nii.gz', suffix='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutdatedExternalDependency",
     "evalue": "No working git-annex installation of version >= 8.20200309. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.. You have version 7.20190819+git2-g908476a9b-1~ndall+1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutdatedExternalDependency\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-0d17968568b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfunc_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'preproc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.nii.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datalad/interface/utils.py\u001b[0m in \u001b[0;36meval_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mlgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Returning return_func from eval_func for %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datalad/interface/utils.py\u001b[0m in \u001b[0;36mreturn_func\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;31m# unwind generator if there is one, this actually runs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0;31m# any processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreturn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'item-or-list'\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                         \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datalad/interface/utils.py\u001b[0m in \u001b[0;36mgenerator_func\u001b[0;34m(*_args, **_kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0mresult_log_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                     \u001b[0;31m# let renderers get to see how a command was called\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m                     allkwargs):\n\u001b[0m\u001b[1;32m    370\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                     \u001b[0;31m# run the hooks before we yield the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datalad/interface/utils.py\u001b[0m in \u001b[0;36m_process_results\u001b[0;34m(results, cmd_class, on_failure, action_summary, incomplete_results, result_renderer, result_log_level, allkwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'action'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;31m# XXX Yarik has to no clue on how to track the origin of the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datalad/distribution/get.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(path, source, dataset, recursive, recursion_limit, get_data, description, reckless, jobs)\u001b[0m\n\u001b[1;32m    972\u001b[0m                     \u001b[0mrefds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m                     \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m                     jobs):\n\u001b[0m\u001b[1;32m    975\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'path'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent_by_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m                     \u001b[0;31m# we had reports on datasets and subdatasets already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datalad/distribution/get.py\u001b[0m in \u001b[0;36m_get_targetpaths\u001b[0;34m(ds, content, refds_path, source, jobs)\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'--from=%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             jobs=jobs)\n\u001b[0m\u001b[1;32m    676\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCommandError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stdout_json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datalad/support/gitrepo.py\u001b[0m in \u001b[0;36m_wrap_normalize_paths\u001b[0;34m(self, files, *args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m             ]\n\u001b[1;32m    322\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msingle_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datalad/support/annexrepo.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, files, remote, options, jobs, key)\u001b[0m\n\u001b[1;32m   1522\u001b[0m             expected_downloads, fetch_files = self._get_expected_files(\n\u001b[1;32m   1523\u001b[0m                 \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'--not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--in'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1524\u001b[0;31m                 \u001b[0mmerge_annex_branches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m  \u001b[0;31m# interested only in local info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1525\u001b[0m             )\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datalad/support/annexrepo.py\u001b[0m in \u001b[0;36m_get_expected_files\u001b[0;34m(self, files, expr, merge_annex_branches)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         for j in self._call_annex_records(\n\u001b[1;32m   1587\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0;34m'find'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1588\u001b[0;31m                 \u001b[0mmerge_annex_branches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_annex_branches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m         ):\n\u001b[1;32m   1590\u001b[0m             \u001b[0;31m# TODO: some files might not even be here.  So in current fancy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datalad/support/annexrepo.py\u001b[0m in \u001b[0;36m_call_annex_records\u001b[0;34m(self, args, files, jobs, git_options, stdin, merge_annex_branches, progress, **kwargs)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mmerge_annex_branches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_annex_branches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCommandError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datalad/support/annexrepo.py\u001b[0m in \u001b[0;36m_call_annex\u001b[0;34m(self, args, files, jobs, protocol, git_options, stdin, merge_annex_branches, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \"\"\"\n\u001b[1;32m    883\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit_annex_version\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_git_annex_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# git portion of the command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/datalad/support/annexrepo.py\u001b[0m in \u001b[0;36m_check_git_annex_version\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mMissingExternalDependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mexc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mver\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGIT_ANNEX_MIN_VERSION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOutdatedExternalDependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mver_present\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit_annex_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutdatedExternalDependency\u001b[0m: No working git-annex installation of version >= 8.20200309. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.. You have version 7.20190819+git2-g908476a9b-1~ndall+1"
     ]
    }
   ],
   "source": [
    "func_paths = [f.path for f in layout.get(subject=subject, desc='preproc', extension='.nii.gz', suffix='bold')]\n",
    "get(func_paths, dataset=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for alejandro: \n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install datalad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Replace this with pybids logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_outputs(dataset_path, sub_id=None, TR=1):\n",
    "    arrays = []\n",
    "    fs = dataset_path.glob(f'fmriprep/sub-{sub_id}/func/*-preproc_bold.nii.gz')\n",
    "    fs = [str(f) for f in fs]\n",
    "    runs = sorted(set([f.split('/')[-1].split('_')[2].split('-')[1] for f in fs]))\n",
    "    for r_ix, r in enumerate(runs):\n",
    "        img_file = [f for f in fs if f'run-{r}' in f][0]\n",
    "        data = np.asanyarray(nib.load(img_file).dataobj)\n",
    "        run_y = data.reshape([data.shape[0] * data.shape[1] * data.shape[2], data.shape[3]]).T\n",
    "        arrays.append(run_y)\n",
    "    return np.vstack(arrays), idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, resampling_ts = _make_outputs(dataset_path, sub_id=subject, TR=TR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3052, 565812)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3052"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([t.shape[0] for t in resampling_ts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the encoding model with lots of comments and printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GroupKFold, PredefinedSplit\n",
    "from himalaya.ridge import GroupRidgeCV\n",
    "from himalaya.scoring import correlation_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validated model fitting, prediction, and scoring loosely based on scikit-learn's [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html). Returns a `results` dictionary with `'coefficients'`, `'test_predictions'`, and `'test_scores'` keys containing lists of numpy arrays for each outer cross-validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _model_cv(estimator, X, y, groups=None,\n",
    "              scoring=correlation_score, cv=None,\n",
    "              inner_cv=None, confounds=None, split=False):\n",
    "\n",
    "    # Container for results\n",
    "    results = {\n",
    "        'coefficients': [],\n",
    "        'test_predictions': [],\n",
    "        'test_scores': []}\n",
    "    \n",
    "    # Extract number of samples for convenience\n",
    "    n_samples = y.shape[0]\n",
    "    \n",
    "    # Keep count of confounds (if provided) for later\n",
    "    n_confounds = confounds.shape[1] if confounds is not None else 0\n",
    "    \n",
    "    # If confounds are provided, stack them onto X model\n",
    "    if confounds is not None:\n",
    "        X = X + [confounds] if type(X) == list else np.hstack((X, confounds))\n",
    "    \n",
    "    # Set default cross-validation to KFold if not specified\n",
    "    cv = KFold() if not cv else cv\n",
    "    \n",
    "    # Loop through outer cross-validation folds\n",
    "    for train, test in cv.split(np.arange(n_samples), groups=groups):\n",
    "        \n",
    "        # Get training model for list of model bands\n",
    "        X_train = [x[train] for x in X] if type(X) == list else X[train]\n",
    "        X_test = [x[test] for x in X] if type(X) == list else X[test]\n",
    "        \n",
    "        # Create inner cross-validation loop if specified\n",
    "        if inner_cv:\n",
    "            \n",
    "            # Split inner cross-validation with groups if supplied\n",
    "            inner_groups = np.array(groups)[train] if groups else groups\n",
    "            inner_splits = inner_cv.split(np.arange(n_samples)[train],\n",
    "                                          groups=inner_groups)\n",
    "            \n",
    "            # Update estimator with inner cross-validator\n",
    "            estimator.set_params(cv=inner_splits)\n",
    "            print(np.unique(inner_groups))\n",
    "        \n",
    "        # Fit the regression model on training data\n",
    "        estimator.fit(X_train, y[train])\n",
    "        \n",
    "        # Zero out coefficients for confounds if provided\n",
    "        if confounds is not None:\n",
    "            estimator.coef_[-n_confounds:] = 0\n",
    "        \n",
    "        # Compute predictions with optional splitting by band\n",
    "        test_prediction = estimator.predict(X_test, split=split)\n",
    "        \n",
    "        # Test scores should also optionally split by band\n",
    "        test_score = scoring(y[test], test_prediction)\n",
    "        \n",
    "        # Populate results dictionary\n",
    "        results['coefficients'].append(estimator.coef_)\n",
    "        results['test_predictions'].append(test_prediction)\n",
    "        results['test_scores'].append(test_score)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input data to numpy and fill NaNs\n",
    "X = [mat.fillna(0).to_numpy() for mat in mats]\n",
    "y = Y[:, :100]\n",
    "\n",
    "# Optionally create some dummy confounds\n",
    "confounds = np.random.randn(y.shape[0], 2)\n",
    "\n",
    "# Default estimator should be GroupRidgeCV\n",
    "estimator = GroupRidgeCV(groups='input')\n",
    "\n",
    "# Default cross-validation should be leave-one-run-out\n",
    "n_runs = len(np.unique(idx))\n",
    "cv = GroupKFold(n_splits=n_runs)\n",
    "inner_cv = GroupKFold(n_splits=n_runs - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1433 1434 1435 1436]\n",
      "[........................................] 100% | 2.25 sec | 100 random sampling with cv | \n",
      "[1433 1434 1435 1437]\n",
      "[........................................] 100% | 2.56 sec | 100 random sampling with cv | \n",
      "[1434 1435 1436 1437]\n",
      "[........................................] 100% | 2.71 sec | 100 random sampling with cv | \n",
      "[1433 1434 1436 1437]\n",
      "[........................................] 100% | 2.81 sec | 100 random sampling with cv | \n",
      "[1433 1435 1436 1437]\n",
      "[........................................] 100% | 2.58 sec | 100 random sampling with cv | \n"
     ]
    }
   ],
   "source": [
    "# Run model with specified cross-validation, groups, confounds, and split outputs\n",
    "results = _model_cv(estimator, X, y, cv=cv, inner_cv=inner_cv, groups=idx, confounds=confounds, split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1433 1434 1435 1436]\n",
      "[........................................] 100% | 2.01 sec | 100 random sampling with cv | \n",
      "[1433 1434 1435 1437]\n",
      "[........................................] 100% | 2.28 sec | 100 random sampling with cv | \n",
      "[1434 1435 1436 1437]\n",
      "[........................................] 100% | 2.37 sec | 100 random sampling with cv | \n",
      "[1433 1434 1436 1437]\n",
      "[........................................] 100% | 2.49 sec | 100 random sampling with cv | \n",
      "[1433 1435 1436 1437]\n",
      "[........................................] 100% | 2.77 sec | 100 random sampling with cv | \n"
     ]
    }
   ],
   "source": [
    "results = _model_cv(estimator, X, y, cv=cv, inner_cv=inner_cv, groups=idx, confounds=confounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n",
      "[........................................] 100% | 2.57 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.67 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.38 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.73 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.68 sec | 100 random sampling with cv | \n"
     ]
    }
   ],
   "source": [
    "results = _model_cv(estimator, X, y, cv=KFold(), inner_cv=KFold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100% | 0.10 sec | 100 random sampling with cv | \n",
      "[..................................      ] 86% | 0.10 sec | 100 random sampling with cv | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100% | 0.12 sec | 100 random sampling with cv | \n",
      "[........................................] 100% | 0.11 sec | 100 random sampling with cv | \n",
      "[..................................      ] 85% | 0.09 sec | 100 random sampling with cv | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100% | 0.10 sec | 100 random sampling with cv | \n",
      "[........................................] 100% | 0.10 sec | 100 random sampling with cv | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "results = _model_cv(estimator, X, y, cv=KFold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n",
      "[........................................] 100% | 2.77 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.67 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.78 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.69 sec | 100 random sampling with cv | \n",
      "[None]\n",
      "[........................................] 100% | 2.65 sec | 100 random sampling with cv | \n"
     ]
    }
   ],
   "source": [
    "results = _model_cv(estimator, X, y, inner_cv=KFold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100% | 0.10 sec | 100 random sampling with cv | \n",
      "[...................................     ] 89% | 0.09 sec | 100 random sampling with cv | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100% | 0.10 sec | 100 random sampling with cv | \n",
      "[........................................] 100% | 0.12 sec | 100 random sampling with cv | \n",
      "[............................            ] 71% | 0.08 sec | 100 random sampling with cv | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100% | 0.11 sec | 100 random sampling with cv | \n",
      "[........................................] 100% | 0.11 sec | 100 random sampling with cv | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/himalaya/backend/numpy.py:43: RuntimeWarning: Mean of empty slice.\n",
      "  return array.mean(axis, dtype=np.float64,\n",
      "/Users/snastase/opt/miniconda3/envs/datalad/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "results = _model_cv(estimator, X, y, groups=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1433 1434 1435 1436]\n",
      "[1433 1434 1435 1437]\n",
      "[1434 1435 1436 1437]\n",
      "[1433 1434 1436 1437]\n",
      "[1433 1435 1436 1437]\n"
     ]
    }
   ],
   "source": [
    "# Using single-band (non-banded) model with sklearn RidgeCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "results = _model_cv(RidgeCV(), X[0], y, cv=cv, inner_cv=inner_cv, groups=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate against other workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
